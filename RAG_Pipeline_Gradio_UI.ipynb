{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghavG189/Python-Projects/blob/main/RAG_Pipeline_Gradio_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN7aKB-PNC9y"
      },
      "source": [
        "Install and import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOC-DbWjeQbA"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index llama-index-llms-gemini pymupdf\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install llama-index-retrievers-bm25\n",
        "!pip install gradio\n",
        "!pip install google-genai\n",
        "!pip install llama-index-llms-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmGC_KfeFAlo"
      },
      "source": [
        "Import necessary libraries and setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ-e58ohFL36"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from google import genai\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "import gradio as gr\n",
        "import fitz\n",
        "import os\n",
        "from llama_index.core import Document\n",
        "from typing import List\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzOex8TCGcjN"
      },
      "source": [
        "Set up google API key for Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVsggbU2FWTd"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = \"\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0MCjrImJEJJ"
      },
      "source": [
        "Load PDF and convert to llamaindex format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTAARHyxJHMj"
      },
      "outputs": [],
      "source": [
        "def load_pdf(pdf_path: str) -> List[Document]:\n",
        "    \"\"\"Load a PDF and convert it to LlamaIndex Document format using PyMuPDF.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    documents = []\n",
        "\n",
        "    for i, page in enumerate(doc):\n",
        "        text = page.get_text()\n",
        "        if not text.strip():\n",
        "            continue\n",
        "        documents.append(\n",
        "            Document(\n",
        "                text=text,\n",
        "                metadata={\n",
        "                    \"file_name\": os.path.basename(pdf_path),\n",
        "                    \"page_number\": i + 1,\n",
        "                    \"total_pages\": len(doc)\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "    doc.close()\n",
        "    print(f\"Processed {pdf_path}:\")\n",
        "    print(f\"Extracted {len(documents)} pages with content\")\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2buhK3CKCEk"
      },
      "source": [
        "Initialize Gemini and Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zt2u329J9la"
      },
      "outputs": [],
      "source": [
        "# Initialize Gemini LLM\n",
        "llm = GoogleGenAI(model=\"gemini-2.0-flash\")\n",
        "\n",
        "# Set as default in LlamaIndex\n",
        "Settings.llm = llm\n",
        "\n",
        "# Initialize embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
        "Settings.embed_model = embed_model\n",
        "splitter = SemanticSplitterNodeParser( # Creates semantic splitter with embedding model\n",
        "    buffer_size = 1,\n",
        "    breakpoint_percentile_threshold = 95, # How sensitive to change in meaning\n",
        "    embed_model = embed_model\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMu9R4UiLB4S"
      },
      "source": [
        "Processes PDF and creates vector and keyword indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85aKMeXBLHgB"
      },
      "outputs": [],
      "source": [
        "def process_and_index_pdf(pdf_path):\n",
        "    documents = load_pdf(pdf_path)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "    print(f\"Indexed {len(documents)} document chunks\")\n",
        "    return vector_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJS9NKCTLOFh"
      },
      "source": [
        "Build RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP-51z0-L_lR"
      },
      "outputs": [],
      "source": [
        "def build_rag_pipeline(index):\n",
        "    nodes = list(index.docstore.docs.values()) # Gets all chunks of text that were created when PDF was indexed\n",
        "    num_nodes = len(nodes) # Stores how many chunks there are\n",
        "    safe_top_k = min(2, max(1, num_nodes)) # Retrieves the minimum value for top k\n",
        "\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=safe_top_k) # Uses embeddings to find chunks that are semantically similar\n",
        "    bm25_retriever = BM25Retriever.from_defaults( # Uses keyword search to find exact terms in chunks found in the query\n",
        "        nodes=nodes,\n",
        "        similarity_top_k=safe_top_k\n",
        "    )\n",
        "\n",
        "    class HybridRetriever(BaseRetriever): #Custom class to combine both vector and keyword search\n",
        "        def __init__(self, vector_retriever, keyword_retriever, top_k=2):\n",
        "            self.vector_retriever = vector_retriever\n",
        "            self.keyword_retriever = keyword_retriever\n",
        "            self.top_k = top_k\n",
        "            super().__init__()\n",
        "\n",
        "        def _retrieve(self, query_bundle, **kwargs):\n",
        "            vector_nodes = self.vector_retriever.retrieve(query_bundle)\n",
        "            keyword_nodes = self.keyword_retriever.retrieve(query_bundle)\n",
        "            all_nodes = list(vector_nodes) + list(keyword_nodes)\n",
        "            unique_nodes = {node.node_id: node for node in all_nodes}\n",
        "            sorted_nodes = sorted(\n",
        "                unique_nodes.values(),\n",
        "                key=lambda x: x.score if hasattr(x, 'score') else 0.0,\n",
        "                reverse=True\n",
        "            )\n",
        "            return sorted_nodes[:self.top_k]\n",
        "\n",
        "    hybrid_retriever = HybridRetriever( # Creates instance of class defined above\n",
        "        vector_retriever=vector_retriever,\n",
        "        keyword_retriever=bm25_retriever,\n",
        "        top_k=safe_top_k\n",
        "    )\n",
        "\n",
        "    if num_nodes > 1:\n",
        "        reranker = SentenceTransformerRerank( #Checks which chunk is most relevant to original query\n",
        "            model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", # More powerful than l-6\n",
        "            top_n=min(2, num_nodes)\n",
        "        )\n",
        "        node_postprocessors = [reranker]\n",
        "    else:\n",
        "        node_postprocessors = []\n",
        "\n",
        "    fusion_retriever = QueryFusionRetriever( #Creates multiple versions of the user's query\n",
        "        retrievers=[hybrid_retriever],\n",
        "        llm=llm,\n",
        "        similarity_top_k=2,\n",
        "        num_queries=3,  # Generate 3 queries per original query\n",
        "        mode=\"reciprocal_rerank\"\n",
        "    )\n",
        "\n",
        "    query_engine = RetrieverQueryEngine.from_args( # Takes fusion retriever and reranker and combines them\n",
        "        retriever=fusion_retriever,\n",
        "        llm=llm,\n",
        "        node_postprocessors=node_postprocessors\n",
        "    )\n",
        "    return query_engine # Returns output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaogYmR9MK0O"
      },
      "source": [
        "Run the Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESm5eC6mMQUk"
      },
      "outputs": [],
      "source": [
        "def rag_chat(pdf_file, user_input, history): #Function for chatting with the chatbot\n",
        "    try:\n",
        "        pdf_path = pdf_file.name #Gets pdf path from pdf file\n",
        "        index = process_and_index_pdf(pdf_path) #Gets the vector index from the process and index function\n",
        "        rag_engine = build_rag_pipeline(index) #Return rag engine object\n",
        "\n",
        "        response = rag_engine.query(user_input) #Gets response from user_input\n",
        "\n",
        "        history = history or [] #Creates history list\n",
        "\n",
        "        history.append((user_input, response.response)) #Stores history between chatbot and user\n",
        "\n",
        "        return history\n",
        "    except Exception as e: #Fallback in case of error\n",
        "        history = history or []\n",
        "\n",
        "        history.append((user_input, f\"‚ö†Ô∏è Error: {str(e)}\"))\n",
        "\n",
        "        return history\n",
        "\n",
        "def handle_feedback(chat_history, data: gr.LikeData):\n",
        "    if not chat_history:\n",
        "        return chat_history  # No messages yet\n",
        "\n",
        "    # Determine message based on like/dislike\n",
        "    feedback_text = \"‚úÖ Thanks for your feedback!\" if data.liked else \"‚ö†Ô∏è Feedback noted.\"\n",
        "\n",
        "    # Append the feedback text to the last bot message\n",
        "    # Changes last bot reply and adds feedback text\n",
        "    # chat_history[:-1] - keeps all chats instead of last one unchanged since we are modifying the recent bot's reply\n",
        "    new_history = chat_history[:-1] + [\n",
        "        (chat_history[-1][0], chat_history[-1][1] + f\"  \\n{feedback_text}\") #gets users reply and bots last message\n",
        "    ]\n",
        "    return new_history\n",
        "\n",
        "#Chat UI\n",
        "with gr.Blocks(title=\"Step 5: Full Functioning UI\") as demo:\n",
        "    gr.Markdown(\"### Step 5: Connected UI\")\n",
        "    with gr.Row():\n",
        "      with gr.Column(scale=2):\n",
        "          chatbot = gr.Chatbot(label=\"Chat History\", height=300)\n",
        "          user_input = gr.Textbox(\n",
        "              placeholder=\"Ask a question about your document...\",\n",
        "              label=\"Your Question\"\n",
        "          )\n",
        "          #Creates send and clear button to be used in UI\n",
        "          send_btn = gr.Button(\"üì§ Send\")\n",
        "          clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\")\n",
        "\n",
        "      with gr.Column(scale=1):\n",
        "          pdf_input = gr.File(label=\"üìÑ Upload PDF\", file_types=[\".pdf\"]) #Allows for pdf upload - replaces uploaded = files.upload()\n",
        "\n",
        "      send_btn.click( #Send button to send query\n",
        "          fn=rag_chat,\n",
        "          inputs=[pdf_input, user_input, chatbot],\n",
        "          outputs=chatbot\n",
        "      )\n",
        "      user_input.submit( #Press Enter to submit query - alternative\n",
        "          fn=rag_chat,\n",
        "          inputs=[pdf_input, user_input, chatbot],\n",
        "          outputs=chatbot\n",
        "      )\n",
        "      clear_btn.click(lambda: [], outputs=chatbot) #Clears chat\n",
        "      chatbot.like(handle_feedback, inputs=[chatbot], outputs=chatbot)\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+8zPt3w91WRylzFUyGeTV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "619f9f28"
      },
      "source": [
        "## üì¶ Setup and Dependencies\n",
        "Install all necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlhtZDjOuPPA"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index-llms-google-genai\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q llama-index\n",
        "!pip install -q pymupdf\n",
        "!apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install opencv-python\n",
        "!pip install pillow\n",
        "!pip install sentence-transformers\n",
        "!pip install gradio\n",
        "!pip install llama-index-retrievers-bm25\n",
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîß Core Imports and Configuration"
      ],
      "metadata": {
        "id": "_4JU7MVKwVD-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj1-L6eZwgg"
      },
      "source": [
        "from google.colab import files\n",
        "from google import genai\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "import gradio as gr\n",
        "import fitz\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import json\n",
        "from llama_index.core import Document\n",
        "from typing import List\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76d6986b"
      },
      "source": [
        "## üß† LLM and Embedding Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Google API key\n",
        "GOOGLE_API_KEY = \"AIzaSyCSADKKVIoz7Hu7jPyuzXcOmgAJLTiL3yQ\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = GoogleGenAI(model=\"gemini-2.0-flash\")\n",
        "\n",
        "# Set as default in LlamaIndex\n",
        "Settings.llm = llm\n",
        "\n",
        "# The embedding model setup remains the same\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "sbYKzRG66xEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82df31c4"
      },
      "source": [
        "## üí° Document Intelligence Functions\n",
        "These functions handle document classification, boundary detection, prediected query type:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_document_type(text: str, max_length: int = 1500) -> str:\n",
        "    \"\"\"\n",
        "    Classify the document type based on its content.\n",
        "    Uses LLM to intelligently identify document category.\n",
        "    \"\"\"\n",
        "\n",
        "    text_sample = text[:max_length] if len(text) > max_length else text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze this document and classify it into ONE of these categories:\n",
        "    - Resume: CV, professional profile, work history\n",
        "    - Contract: Legal agreement, terms and conditions, service agreement\n",
        "    - Mortgage Contract: Home loan agreement, mortgage terms, property financing\n",
        "    - Invoice: Bill, payment request, financial statement\n",
        "    - Pay Slip: Salary statement, wage slip, earnings statement\n",
        "    - Lender Fee Sheet: Loan fees, lender charges, closing costs\n",
        "    - Land Deed: Property deed, title document, ownership certificate\n",
        "    - Bank Statement: Account statement, transaction history\n",
        "    - Tax Document: W2, 1099, tax return, tax form\n",
        "    - Insurance: Insurance policy, coverage document\n",
        "    - Report: Analysis, research document, findings\n",
        "    - Letter: Correspondence, memo, communication\n",
        "    - Form: Application, questionnaire, data entry form\n",
        "    - ID Document: Driver's license, passport, identification\n",
        "    - Medical: Medical report, prescription, health record\n",
        "    - Other: Doesn't fit other categories\n",
        "\n",
        "    Document sample:\n",
        "    {text_sample}\n",
        "\n",
        "    Respond with ONLY the category name, nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.complete(prompt)\n",
        "        doc_type = response.text.strip()\n",
        "\n",
        "        # Normalize the response\n",
        "        valid_types = [\n",
        "            'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',\n",
        "            'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',\n",
        "            'Insurance', 'Report', 'Letter', 'Form', 'ID Document',\n",
        "            'Medical', 'Other'\n",
        "        ]\n",
        "\n",
        "        # Find best match (case-insensitive)\n",
        "        for valid_type in valid_types:\n",
        "            if doc_type.lower() == valid_type.lower():\n",
        "                return valid_type\n",
        "\n",
        "        return 'Other'\n",
        "    except Exception as e:\n",
        "        print(f\"Classification error: {e}\")\n",
        "        return 'Other'"
      ],
      "metadata": {
        "id": "1sTWrl5kwVEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_document_boundary(prev_text: str, curr_text: str,\n",
        "                            current_doc_type: str = None) -> bool:\n",
        "    \"\"\"\n",
        "    Detect if two consecutive pages belong to the same document.\n",
        "    Returns True if they're from the same document.\n",
        "    \"\"\"\n",
        "    # Quick heuristic checks first\n",
        "    if not prev_text or not curr_text:\n",
        "        return False\n",
        "\n",
        "    # Sample the texts for LLM analysis\n",
        "    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text\n",
        "    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Determine if these two pages are from the SAME document.\n",
        "\n",
        "    Current document type: {current_doc_type or 'Unknown'}\n",
        "\n",
        "    End of Previous Page:\n",
        "    ...{prev_sample}\n",
        "\n",
        "    Start of Current Page:\n",
        "    {curr_sample}...\n",
        "\n",
        "    Consider:\n",
        "    - Continuity of content\n",
        "    - Formatting consistency\n",
        "    - Topic coherence\n",
        "    - Page numbers or headers\n",
        "\n",
        "    Answer ONLY 'Yes' if same document or 'No' if different document.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.complete(prompt)\n",
        "        return response.text.strip().lower().startswith('yes')\n",
        "    except Exception as e:\n",
        "        print(f\"Boundary detection error: {e}\")\n",
        "        # Default to keeping pages together if uncertain\n",
        "        return True"
      ],
      "metadata": {
        "id": "dRv0ILbPj3bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_query_document_type(query: str):\n",
        "    \"\"\"\n",
        "    Predict which document type is most likely to contain the answer.\n",
        "    Returns predicted type and confidence score.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analyze this query and predict which document type would most likely contain the answer.\n",
        "\n",
        "    Query: \"{query}\"\n",
        "\n",
        "    Choose the MOST LIKELY type from:\n",
        "    - Resume: Career, experience, education, skills, employment history\n",
        "    - Contract: Terms, agreements, obligations, parties, legal terms\n",
        "    - Mortgage Contract: Home loan, property financing, mortgage terms, interest rates\n",
        "    - Invoice: Payments, amounts due, billing, charges, invoiced items\n",
        "    - Pay Slip: Salary, wages, deductions, earnings, pay period\n",
        "    - Lender Fee Sheet: Loan fees, closing costs, origination fees, lender charges\n",
        "    - Land Deed: Property ownership, deed information, property description, title\n",
        "    - Bank Statement: Account balance, transactions, deposits, withdrawals\n",
        "    - Tax Document: Tax information, W2, 1099, tax returns, tax amounts\n",
        "    - Insurance: Coverage, policy details, premiums, claims\n",
        "    - Report: Analysis, findings, conclusions, research data\n",
        "    - Letter: Communications, requests, notifications, correspondence\n",
        "    - Form: Applications, submitted data, form fields\n",
        "    - ID Document: Personal identification, ID numbers, identity verification\n",
        "    - Medical: Health information, medical conditions, prescriptions\n",
        "    - Other: General or unclear\n",
        "\n",
        "    Respond in JSON format:\n",
        "    {{\"type\": \"DocumentType\", \"confidence\": 0.85}}\n",
        "\n",
        "    Confidence should be between 0.0 and 1.0\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.complete(prompt)\n",
        "        result = json.loads(response.text.strip())\n",
        "        return result.get(\"type\", \"Other\"), result.get(\"confidence\", 0.5)\n",
        "    except Exception as e:\n",
        "        print(f\"Query routing error: {e}\")\n",
        "        return \"Other\", 0.0"
      ],
      "metadata": {
        "id": "7grc1Dixj3lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddac361"
      },
      "source": [
        "## ‚ú® Image Preprocessing for OCR\n",
        "Functions to enhance images for better text extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ce75693"
      },
      "source": [
        "# =======================\n",
        "# üìå Preprocess Image for OCR\n",
        "# =======================\n",
        "def preprocess_image(img, show_preview=False):\n",
        "    \"\"\"\n",
        "    Preprocess an image for OCR by enhancing contrast and sharpening.\n",
        "    Uses CLAHE for local contrast normalization and Laplacian filtering for edge enhancement.\n",
        "    \"\"\"\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Apply CLAHE to enhance local contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8, 8))\n",
        "    gray = clahe.apply(gray)\n",
        "\n",
        "    # Apply a mild sharpening filter to enhance edges\n",
        "    sharpening_kernel = np.array([\n",
        "        [0, -1, 0],\n",
        "        [-1, 4.5, -1],\n",
        "        [0, -1, 0]\n",
        "    ])\n",
        "    gray = cv2.filter2D(gray, -1, sharpening_kernel)\n",
        "\n",
        "    # Resize for better OCR accuracy (Tesseract works better on larger text)\n",
        "    scale_percent = 200  # Increase image size by 200%\n",
        "    width = int(gray.shape[1] * scale_percent / 100)\n",
        "    height = int(gray.shape[0] * scale_percent / 100)\n",
        "    gray = cv2.resize(gray, (width, height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    return gray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fbd8985"
      },
      "source": [
        "## üìÑ Document Ingestion and Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_analyze(pdf_path):\n",
        "  doc = fitz.open(pdf_path)\n",
        "  pages_info = []\n",
        "\n",
        "  for i, page in enumerate(doc):\n",
        "    text = page.get_text()\n",
        "\n",
        "    if not text.strip():\n",
        "      print(f\"  Page {i}: No text found, attempting OCR...\")\n",
        "\n",
        "      try:\n",
        "        pix = page.get_pixmap(dpi=300) # Increase DPI for better OCR\n",
        "        img = np.array(Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples))\n",
        "\n",
        "        # Use the new preprocess_image function\n",
        "        preprocessed_img = preprocess_image(img)\n",
        "\n",
        "        custom_config = r'--oem 3 -l eng'\n",
        "        text = pytesseract.image_to_string(preprocessed_img, config=custom_config)\n",
        "\n",
        "        print(f\"  Page {i}: OCR extracted {len(text)} characters\")\n",
        "      except Exception as e:\n",
        "        print(f\"OCR Failed - {e} for Page {i}\")\n",
        "        text = \"\"\n",
        "\n",
        "    #Applies metadata to each page of text\n",
        "    pages_info.append(\n",
        "          Document(\n",
        "              text=text,\n",
        "              metadata={\n",
        "                  \"page_number\": i,\n",
        "                  \"doc_type\": None,\n",
        "                  \"page_in_doc\": None,\n",
        "                  \"source\": pdf_path\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "  doc.close()\n",
        "\n",
        "  if not pages_info:\n",
        "    raise ValueError(\"No text could be extracted from PDF\")\n",
        "\n",
        "  print(f\"‚úÖ Extracted {len(pages_info)} pages\")\n",
        "\n",
        "  doc_counter = 0\n",
        "  current_doc_type = None\n",
        "\n",
        "  #Uses current_doc_type and classify_document_type to fill in reast of metadata\n",
        "  for i, page_info in enumerate(pages_info):\n",
        "\n",
        "    if i == 0:\n",
        "      current_doc_type = classify_document_type(page_info.text)\n",
        "      page_info.metadata[\"doc_type\"] = current_doc_type\n",
        "      page_info.metadata[\"page_in_doc\"] = 0\n",
        "    else:\n",
        "      prev_text = pages_info[i-1].text\n",
        "      curr_text = page_info.text\n",
        "\n",
        "      same = detect_document_boundary(prev_text, curr_text, current_doc_type)\n",
        "\n",
        "      if not same:\n",
        "        doc_counter = 0\n",
        "        page_info.metadata[\"page_in_doc\"] = doc_counter\n",
        "        current_doc_type = classify_document_type(page_info.text)\n",
        "        page_info.metadata[\"doc_type\"] = current_doc_type\n",
        "      else:\n",
        "        doc_counter += 1\n",
        "        page_info.metadata[\"page_in_doc\"] = doc_counter\n",
        "        page_info.metadata[\"doc_type\"] = current_doc_type\n",
        "\n",
        "  return pages_info"
      ],
      "metadata": {
        "id": "m8UGXbJ3dhiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_index_pdf(pdf_path):\n",
        "    documents = extract_and_analyze(pdf_path)\n",
        "    all_nodes = []\n",
        "    for doc in documents:\n",
        "        # Use chunk_page_with_metadata on each document (which represents a page)\n",
        "        nodes_from_page = chunk_page_with_metadata(doc)\n",
        "        all_nodes.extend(nodes_from_page)\n",
        "\n",
        "    # Build the index from all the collected nodes\n",
        "    vector_index = VectorStoreIndex(all_nodes)\n",
        "    print(f\"Indexed {len(all_nodes)} document chunks\")\n",
        "    return vector_index"
      ],
      "metadata": {
        "id": "wkg6UF_s88_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3a50a06"
      },
      "source": [
        "## ‚úÇÔ∏è Chunking and Metadata Assignment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_page_with_metadata(page_info: Document, chunk_size: int = 500, chunk_overlap: int = 100):\n",
        "\n",
        "  splitter = SemanticSplitterNodeParser( # Creates semantic splitter with embedding model\n",
        "    buffer_size = 1,\n",
        "    breakpoint_percentile_threshold = 95, # How sensitive to change in meaning\n",
        "    embed_model = embed_model\n",
        "  )\n",
        "\n",
        "  # Create nodes(chunks) from documents\n",
        "  # Pass the single Document object (page_info) as a list to get_nodes_from_documents\n",
        "  nodes = splitter.get_nodes_from_documents([page_info])\n",
        "\n",
        "  # Assign additional metadata to the LlamaIndex Node objects\n",
        "  for i, node in enumerate(nodes):\n",
        "      # Add metadata from the original page_info Document\n",
        "      node.metadata[\"page_number\"] = page_info.metadata.get(\"page_number\")\n",
        "      node.metadata[\"doc_type\"] = page_info.metadata.get(\"doc_type\")\n",
        "      node.metadata[\"page_in_doc\"] = page_info.metadata.get(\"page_in_doc\")\n",
        "      node.metadata[\"source\"] = page_info.metadata.get(\"source\")\n",
        "      # Add chunk-specific metadata\n",
        "      node.metadata[\"chunk_index\"] = i\n",
        "      node.metadata[\"doc_id\"] = page_info.id_ # Use the Document's id as the base doc_id for chunks\n",
        "      node.id_ = f\"{page_info.id_}_chunk_{i}\" # Assign a unique ID to the node\n",
        "\n",
        "  return nodes # Return the list of LlamaIndex Node objects"
      ],
      "metadata": {
        "id": "0HMbuKip2Lnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d85eeb7e"
      },
      "source": [
        "## üèóÔ∏è RAG Pipeline Construction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rag_pipeline(index, predicted_doc_type: str = None, num_chunks: int = 4):\n",
        "    nodes = list(index.docstore.docs.values())\n",
        "    num_nodes = len(nodes)\n",
        "    safe_top_k = min(num_chunks, max(1, num_nodes)) # Use num_chunks from input\n",
        "\n",
        "    # --- Add Metadata Filtering ---\n",
        "    from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters\n",
        "\n",
        "    filters = None\n",
        "    if predicted_doc_type and predicted_doc_type != \"Other\":\n",
        "        print(f\"Applying filter for document type: {predicted_doc_type}\")\n",
        "        filters = MetadataFilters(\n",
        "            filters=[ExactMatchFilter(key=\"doc_type\", value=predicted_doc_type)]\n",
        "        )\n",
        "\n",
        "    # Use the filter in the vector retriever\n",
        "    vector_retriever = index.as_retriever(\n",
        "        similarity_top_k=safe_top_k,\n",
        "        filters=filters # This is the new part!\n",
        "    )\n",
        "    # --- End of Filtering Logic ---\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_defaults(\n",
        "        nodes=nodes,\n",
        "        similarity_top_k=safe_top_k\n",
        "    )\n",
        "\n",
        "\n",
        "    class HybridRetriever(BaseRetriever): #Custom class to combine both vector and keyword search\n",
        "\n",
        "     def __init__(self, vector_retriever, keyword_retriever, top_k=2):\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.keyword_retriever = keyword_retriever\n",
        "        self.top_k = top_k\n",
        "        super().__init__()\n",
        "     def _retrieve(self, query_bundle, **kwargs):\n",
        "        vector_nodes = self.vector_retriever.retrieve(query_bundle)\n",
        "        keyword_nodes = self.keyword_retriever.retrieve(query_bundle)\n",
        "        all_nodes = list(vector_nodes) + list(keyword_nodes)\n",
        "        unique_nodes = {node.node_id: node for node in all_nodes}\n",
        "        sorted_nodes = sorted(\n",
        "           unique_nodes.values(),\n",
        "           key=lambda x: x.score if hasattr(x, 'score') else 0.0,\n",
        "           reverse=True\n",
        "        )\n",
        "        return sorted_nodes[:self.top_k]\n",
        "\n",
        "    hybrid_retriever = HybridRetriever( # Creates instance of class defined above\n",
        "      vector_retriever=vector_retriever,\n",
        "      keyword_retriever=bm25_retriever,\n",
        "      top_k=safe_top_k\n",
        "    )\n",
        "\n",
        "    if num_nodes > 1:\n",
        "      reranker = SentenceTransformerRerank( #Checks which chunk is most relevant to original query\n",
        "         model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", # More powerful than l-6\n",
        "         top_n=min(2, num_nodes) # Use a smaller top_n for reranker, typically 2 is enough\n",
        "      )\n",
        "      node_postprocessors = [reranker]\n",
        "    else:\n",
        "      node_postprocessors = []\n",
        "\n",
        "    fusion_retriever = QueryFusionRetriever( #Creates multiple versions of the user's query\n",
        "      retrievers=[hybrid_retriever],\n",
        "      llm=llm,\n",
        "      similarity_top_k=safe_top_k, # Use num_chunks here as well\n",
        "      num_queries=3,  # Generate 3 queries per original query\n",
        "      mode=\"reciprocal_rerank\"\n",
        "    )\n",
        "\n",
        "    query_engine = RetrieverQueryEngine.from_args( # Takes fusion retriever and reranker and combines them\n",
        "        retriever=fusion_retriever,\n",
        "        llm=llm,\n",
        "        node_postprocessors=node_postprocessors\n",
        "    )\n",
        "    return query_engine # Returns output"
      ],
      "metadata": {
        "id": "bsDcsVId_JE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01b2e2f"
      },
      "source": [
        "## üí¨ Gradio Chat Interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable to store the index and document info after processing.\n",
        "# This ensures we only process the PDF once.\n",
        "global_index = None\n",
        "document_info = {\n",
        "    \"doc_types\": [\"All\"],\n",
        "    \"structure\": \"Document structure will appear here.\"\n",
        "}\n",
        "\n",
        "def process_pdf_for_gradio(pdf_file):\n",
        "    \"\"\"\n",
        "    Handles the PDF processing when the user clicks the 'Process' button.\n",
        "    It processes the PDF, builds the index, and extracts metadata for the UI.\n",
        "    \"\"\"\n",
        "    global global_index, document_info\n",
        "\n",
        "    if pdf_file is None:\n",
        "        return \"‚ö†Ô∏è Please upload a PDF file first.\", \"Document structure will appear here.\", gr.update(choices=[\"All\"], value=\"All\")\n",
        "\n",
        "    try:\n",
        "        # Store the index in the global variable so we don't have to re-process\n",
        "        index = process_and_index_pdf(pdf_file.name)\n",
        "        global_index = index\n",
        "\n",
        "        # --- Extract document structure and types for the UI ---\n",
        "        nodes = list(index.docstore.docs.values())\n",
        "        doc_types = sorted(list(set(node.metadata.get(\"doc_type\", \"N/A\") for node in nodes)))\n",
        "\n",
        "        structure_summary = {}\n",
        "        for doc_type in doc_types:\n",
        "            pages = sorted(list(set(\n",
        "                node.metadata.get(\"page_number\", -1) + 1\n",
        "                for node in nodes if node.metadata.get(\"doc_type\") == doc_type\n",
        "            )))\n",
        "            page_str = \", \".join(map(str, pages))\n",
        "            structure_summary[doc_type] = f\"Pages: {page_str}\"\n",
        "\n",
        "        structure_display = \"### Document Structure\\n\" + \"\\n\".join(\n",
        "            f\"- **{dtype}**: {info}\" for dtype, info in structure_summary.items()\n",
        "        )\n",
        "\n",
        "        # Store info and prepare UI updates\n",
        "        document_info['doc_types'] = [\"All\"] + doc_types\n",
        "        document_info['structure'] = structure_display\n",
        "\n",
        "        status_msg = f\"‚úÖ Successfully processed **{os.path.basename(pdf_file.name)}**. Ready to chat.\"\n",
        "\n",
        "        return status_msg, structure_display, gr.update(choices=document_info['doc_types'], value=\"All\")\n",
        "\n",
        "    except Exception as e:\n",
        "        global_index = None\n",
        "        error_msg = f\"‚ùå **Error processing PDF:** {str(e)}\"\n",
        "        return error_msg, \"Failed to process.\", gr.update(choices=[\"All\"], value=\"All\")"
      ],
      "metadata": {
        "id": "0NlFscTf6p5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_handler(user_input, history, num_chunks, auto_route, doc_filter):\n",
        "    \"\"\"\n",
        "    Handles the chat logic. Now formats output for type='messages'.\n",
        "    \"\"\"\n",
        "    # Don't do anything if the user input is empty\n",
        "    if not user_input or not user_input.strip():\n",
        "        return history\n",
        "\n",
        "    # Append the user's message to the history in the new dictionary format\n",
        "    history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Handle the case where no document is processed\n",
        "    if global_index is None:\n",
        "        history.append({\"role\": \"assistant\", \"content\": \"üìö Please upload and process a PDF document first.\"})\n",
        "        return history\n",
        "\n",
        "    try:\n",
        "\n",
        "        predicted_doc_type = None\n",
        "        confidence_pred = 0.0\n",
        "\n",
        "        if auto_route and doc_filter == \"All\":\n",
        "            predicted_doc_type, confidence_pred = predict_query_document_type(user_input)\n",
        "            print(f\"Predicted query document type: {predicted_doc_type} with confidence {confidence_pred:.2f}\")\n",
        "        elif doc_filter != \"All\":\n",
        "            predicted_doc_type = doc_filter\n",
        "            print(f\"Using manual filter for document type: {predicted_doc_type}\")\n",
        "\n",
        "        rag_engine = build_rag_pipeline(\n",
        "            global_index,\n",
        "            predicted_doc_type=predicted_doc_type,\n",
        "            num_chunks=int(num_chunks)\n",
        "        )\n",
        "\n",
        "        response = rag_engine.query(user_input)\n",
        "\n",
        "        sources = []\n",
        "        if hasattr(response, 'source_nodes'):\n",
        "            for node in response.source_nodes:\n",
        "                score = node.score\n",
        "                chunk_text = node.get_content()\n",
        "                metadata = node.metadata\n",
        "                page_num = metadata.get('page_number', -1)\n",
        "                doc_type_info = metadata.get('doc_type', 'N/A')\n",
        "\n",
        "                # Format the new, more detailed source string\n",
        "                source_info = (\n",
        "                    f\"- **{doc_type_info}** (Page: {page_num + 1}) [Score: {score:.2f}]\\n\"\n",
        "                    f\"  > \\\"{chunk_text[:250]}...\\\"\"  # Show a 250-character preview\n",
        "                )\n",
        "                sources.append(source_info)\n",
        "\n",
        "        source_text = \"\\n\\nüìç **Sources:**\\n\" + \"\\n\".join(sources) if sources else \"\"\n",
        "        formatted_response = f\"{response.response}{source_text}\"\n",
        "        # --- End of original logic ---\n",
        "\n",
        "        # Append the bot's successful response\n",
        "        history.append({\"role\": \"assistant\", \"content\": formatted_response})\n",
        "\n",
        "    except Exception as e:\n",
        "        # Append an error message if something goes wrong\n",
        "        error_message = f\"‚ö†Ô∏è An error occurred: {str(e)}\"\n",
        "        history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "2T2QnJAH8BOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Gradio UI Blocks ---\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Enhanced Document RAG\") as demo:\n",
        "    gr.Markdown(\"## üöÄ Document RAG with Enhanced Processing\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            pdf_input = gr.File(label=\"1. Upload PDF\", file_types=[\".pdf\"])\n",
        "\n",
        "            # A dedicated button to trigger processing\n",
        "            process_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\")\n",
        "\n",
        "            gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
        "            doc_type_filter = gr.Dropdown(\n",
        "                choices=[\"All\"], value=\"All\", label=\"Filter by Document Type\"\n",
        "            )\n",
        "            auto_route_toggle = gr.Checkbox(\n",
        "                value=True, label=\"Enable Auto Document Routing\"\n",
        "            )\n",
        "            num_chunks_input = gr.Slider(\n",
        "                minimum=1, maximum=10, value=4, step=1, label=\"Chunks to Retrieve (k)\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### üìä Processing Status\")\n",
        "            process_status = gr.Markdown(\"‚è≥ Waiting for document to be processed...\")\n",
        "            document_structure = gr.Markdown()\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(label=\"Chat History\", height=600, type=\"messages\")\n",
        "\n",
        "            with gr.Row():\n",
        "                user_input = gr.Textbox(\n",
        "                    placeholder=\"Ask a question about your document...\",\n",
        "                    label=\"2. Ask a Question\",\n",
        "                    scale=4\n",
        "                )\n",
        "                send_btn = gr.Button(\"üì§ Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\")\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "\n",
        "    # 1. When the \"Process\" button is clicked\n",
        "    process_btn.click(\n",
        "        fn=process_pdf_for_gradio,\n",
        "        inputs=[pdf_input],\n",
        "        outputs=[process_status, document_structure, doc_type_filter]\n",
        "    )\n",
        "\n",
        "    # 2. When the \"Send\" button is clicked for chat\n",
        "    send_btn.click(\n",
        "        fn=chat_handler,\n",
        "        inputs=[user_input, chatbot, num_chunks_input, auto_route_toggle, doc_type_filter],\n",
        "        outputs=[chatbot]\n",
        "    ).then(lambda: gr.update(value=\"\"), outputs=[user_input]) # Clears the textbox after sending\n",
        "\n",
        "    # 3. When the user presses Enter in the textbox\n",
        "    user_input.submit(\n",
        "        fn=chat_handler,\n",
        "        inputs=[user_input, chatbot, num_chunks_input, auto_route_toggle, doc_type_filter],\n",
        "        outputs=[chatbot]\n",
        "    ).then(lambda: gr.update(value=\"\"), outputs=[user_input]) # Clears the textbox after sending\n",
        "\n",
        "    # 4. Clear chat button\n",
        "    clear_btn.click(lambda: [], outputs=[chatbot])\n",
        "# This line starts the user interface\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "NrbQhFxK8BD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}